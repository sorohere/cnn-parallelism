Script started on Thu Nov 13 12:17:27 2025
[1m[7m%[27m[1m[0m                                                                                                                                            [0m[27m[24m[Jsaurabh@sORo cnn-parallelism % [K[?2004hmmak e   m  [7mmake benchmark_detailed[27m[23D[27mm[27ma[27mk[27me[27m [27mb[27me[27mn[27mc[27mh[27mm[27ma[27mr[27mk[27m_[27md[27me[27mt[27ma[27mi[27ml[27me[27md[?2004l

==========================================================================
âœ“ All inference programs compiled successfully!
==========================================================================

Available executables:
  ./serial_inference                - Serial baseline
  ./data_parallel_inference         - Data parallel (MPI)
  ./pipeline_parallel_inference     - Pipeline parallel (MPI)

Next step: make benchmark
==========================================================================
================================================================================
     ENHANCED CNN INFERENCE BENCHMARK WITH DETAILED PERFORMANCE METRICS        
================================================================================

Timestamp: 2025-11-13 12:17:42
System: Darwin arm64
CPU cores: 8
CPU model: Apple M2
RAM: 16 GB

[0;34m[Step 1/5] Checking prerequisites...[0m
[0;32mâœ“ All prerequisites met[0m

[0;34m[Step 2/5] Running Serial Inference (Baseline)...[0m
==================================================================================
=================================================
   SERIAL CNN INFERENCE (Baseline Performance)  
=================================================

[1/5] Initializing CNN layers...
    âœ“ Network initialized: Input(1Ã—28Ã—28) â†’ Conv1(16Ã—14Ã—14) â†’ Conv2(32Ã—7Ã—7) â†’ FC1(200) â†’ FC2(200) â†’ Output(10)
    âœ“ Layer creation time: 0.010 seconds

[2/5] Loading pre-trained model weights...
    âœ“ Model weights loaded successfully
    âœ“ Model load time: 0.003 seconds

[3/5] Loading MNIST test dataset...
    âœ“ Loaded 10000 test images
    âœ“ Data load time: 0.005 seconds

[4/5] Running serial inference on single CPU core...
    (Processing 10000 images sequentially)

    Progress: 1000/10000 images (10.0%)
    Progress: 2000/10000 images (20.0%)
    Progress: 3000/10000 images (30.0%)
    Progress: 4000/10000 images (40.0%)
    Progress: 5000/10000 images (50.0%)
    Progress: 6000/10000 images (60.0%)
    Progress: 7000/10000 images (70.0%)
    Progress: 8000/10000 images (80.0%)
    Progress: 9000/10000 images (90.0%)
    Progress: 10000/10000 images (100.0%)

[5/5] Results:

========================================================================
  SERIAL INFERENCE - PERFORMANCE SUMMARY
========================================================================
  Execution Metrics:
    Total Time:              5.294 seconds
    Inference Time:          5.276 seconds
    Model Load Time:         0.003 seconds
    Data Load Time:          0.005 seconds

  Layer-wise Timing:

  Throughput & Latency:
    Throughput:              1895.44 images/second
    Avg Latency per Image:   0.528 ms
    Min Latency:             0.520 ms
    Max Latency:             1.109 ms

  Memory Usage:
    Peak Memory:             11.77 MB

  Accuracy:
    Correct Predictions:     9732 / 10000
    Accuracy:                97.32%
========================================================================

DETAILED ANALYSIS:
------------------

  Time Distribution:

Performance Baseline:
  This is SERIAL execution (1 CPU core)
  Use this as baseline for parallel comparison

[0;32mâœ“ Serial inference completed[0m

[0;34m[Step 3/5] Running Data Parallel Inference (MPI)...[0m
==================================================================================

[1;33mTesting with 1 processes...[0m
i=0
i=1000
i=2000
i=3000
i=4000
i=5000
i=6000
i=7000
i=8000
i=9000


========================================================================
  DATA PARALLEL INFERENCE - PERFORMANCE SUMMARY
========================================================================
  Execution Metrics:
    Total Time:              5.280 seconds
    Inference Time:          5.269 seconds
    Model Load Time:         0.010 seconds
    Data Load Time:          0.001 seconds
    Communication Time:      0.000 seconds (0.0%)
      - MPI Send Time:       0.000 seconds
      - MPI Recv Time:       0.000 seconds
      - MPI Wait Time:       0.000 seconds

  Layer-wise Timing:

  Throughput & Latency:
    Throughput:              1898.05 images/second
    Avg Latency per Image:   0.527 ms
    Min Latency:             0.520 ms
    Max Latency:             0.816 ms

  Memory Usage:
    Peak Memory:             21.95 MB

  Accuracy:
    Correct Predictions:     9732 / 10000
    Accuracy:                97.32%
========================================================================

DETAILED ANALYSIS:
------------------

  Time Distribution:

Load Balancing Analysis:
  Max Process Time:        5.269 seconds
  Min Process Time:        5.269 seconds
  Time Variance:           0.000 seconds
  Load Imbalance Factor:   0.00%

  âœ“ Excellent load balance (< 5% imbalance)

[0;32mâœ“ Data parallel with 1 processes completed[0m
  â†’ Speedup: 1.00x, Efficiency: 100.0%


[1;33mTesting with 2 processes...[0m
i=0
i=1000
i=2000
i=3000
i=4000


========================================================================
  DATA PARALLEL INFERENCE - PERFORMANCE SUMMARY
========================================================================
  Execution Metrics:
    Total Time:              2.779 seconds
    Inference Time:          2.767 seconds
    Model Load Time:         0.011 seconds
    Data Load Time:          0.001 seconds
    Communication Time:      0.000 seconds (0.0%)
      - MPI Send Time:       0.000 seconds
      - MPI Recv Time:       0.000 seconds
      - MPI Wait Time:       0.000 seconds

  Layer-wise Timing:

  Throughput & Latency:
    Throughput:              3613.99 images/second
    Avg Latency per Image:   0.277 ms
    Min Latency:             0.548 ms
    Max Latency:             0.977 ms

  Memory Usage:
    Peak Memory:             22.39 MB

  Parallelization Metrics:
    Number of Processes:     2
    Speedup:                 0.00x
    Parallel Efficiency:     0.00%
    Load Imbalance:          0.08%

  Accuracy:
    Correct Predictions:     9732 / 10000
    Accuracy:                97.32%
========================================================================

DETAILED ANALYSIS:
------------------
  Computation Time:        2.767 seconds (100.0%)
  Communication Overhead:  0.000 seconds (0.0%)
  Ideal Time (Perfect Scaling): inf seconds
  Scaling Loss:            -inf seconds

  Time Distribution:

Load Balancing Analysis:
  Max Process Time:        2.767 seconds
  Min Process Time:        2.765 seconds
  Time Variance:           0.002 seconds
  Load Imbalance Factor:   0.08%

  âœ“ Excellent load balance (< 5% imbalance)

[0;32mâœ“ Data parallel with 2 processes completed[0m
  â†’ Speedup: 1.90x, Efficiency: 90.0%


[1;33mTesting with 3 processes...[0m
i=0
i=1000
i=2000
i=3000


========================================================================
  DATA PARALLEL INFERENCE - PERFORMANCE SUMMARY
========================================================================
  Execution Metrics:
    Total Time:              1.925 seconds
    Inference Time:          1.912 seconds
    Model Load Time:         0.011 seconds
    Data Load Time:          0.002 seconds
    Communication Time:      0.000 seconds (0.0%)
      - MPI Send Time:       0.000 seconds
      - MPI Recv Time:       0.000 seconds
      - MPI Wait Time:       0.000 seconds

  Layer-wise Timing:

  Throughput & Latency:
    Throughput:              5230.17 images/second
    Avg Latency per Image:   0.191 ms
    Min Latency:             0.569 ms
    Max Latency:             1.121 ms

  Memory Usage:
    Peak Memory:             22.56 MB

  Parallelization Metrics:
    Number of Processes:     3
    Speedup:                 0.00x
    Parallel Efficiency:     0.00%
    Load Imbalance:          0.09%

  Accuracy:
    Correct Predictions:     9732 / 10000
    Accuracy:                97.32%
========================================================================

DETAILED ANALYSIS:
------------------
  Computation Time:        1.912 seconds (100.0%)
  Communication Overhead:  0.000 seconds (0.0%)
  Ideal Time (Perfect Scaling): inf seconds
  Scaling Loss:            -inf seconds

  Time Distribution:

Load Balancing Analysis:
  Max Process Time:        1.912 seconds
  Min Process Time:        1.910 seconds
  Time Variance:           0.002 seconds
  Load Imbalance Factor:   0.09%

  âœ“ Excellent load balance (< 5% imbalance)

[0;32mâœ“ Data parallel with 3 processes completed[0m
  â†’ Speedup: 2.75x, Efficiency: 90.0%


[1;33mTesting with 4 processes...[0m
i=0
i=1000
i=2000


========================================================================
  DATA PARALLEL INFERENCE - PERFORMANCE SUMMARY
========================================================================
  Execution Metrics:
    Total Time:              1.453 seconds
    Inference Time:          1.440 seconds
    Model Load Time:         0.011 seconds
    Data Load Time:          0.002 seconds
    Communication Time:      0.004 seconds (0.3%)
      - MPI Send Time:       0.000 seconds
      - MPI Recv Time:       0.000 seconds
      - MPI Wait Time:       0.000 seconds

  Layer-wise Timing:

  Throughput & Latency:
    Throughput:              6942.93 images/second
    Avg Latency per Image:   0.144 ms
    Min Latency:             0.569 ms
    Max Latency:             1.678 ms

  Memory Usage:
    Peak Memory:             22.59 MB

  Parallelization Metrics:
    Number of Processes:     4
    Speedup:                 0.00x
    Parallel Efficiency:     0.00%
    Load Imbalance:          0.31%

  Accuracy:
    Correct Predictions:     9732 / 10000
    Accuracy:                97.32%
========================================================================

DETAILED ANALYSIS:
------------------
  Computation Time:        1.436 seconds (99.7%)
  Communication Overhead:  0.004 seconds (0.3%)
  Ideal Time (Perfect Scaling): inf seconds
  Scaling Loss:            -inf seconds

  Time Distribution:

Load Balancing Analysis:
  Max Process Time:        1.440 seconds
  Min Process Time:        1.436 seconds
  Time Variance:           0.005 seconds
  Load Imbalance Factor:   0.31%

  âœ“ Excellent load balance (< 5% imbalance)

[0;32mâœ“ Data parallel with 4 processes completed[0m
  â†’ Speedup: 3.66x, Efficiency: 90.0%


[1;33mTesting with 5 processes...[0m
i=0
i=1000


========================================================================
  DATA PARALLEL INFERENCE - PERFORMANCE SUMMARY
========================================================================
  Execution Metrics:
    Total Time:              1.310 seconds
    Inference Time:          1.296 seconds
    Model Load Time:         0.011 seconds
    Data Load Time:          0.001 seconds
    Communication Time:      0.061 seconds (4.6%)
      - MPI Send Time:       0.000 seconds
      - MPI Recv Time:       0.000 seconds
      - MPI Wait Time:       0.000 seconds

  Layer-wise Timing:

  Throughput & Latency:
    Throughput:              7716.50 images/second
    Avg Latency per Image:   0.130 ms
    Min Latency:             0.569 ms
    Max Latency:             4.138 ms

  Memory Usage:
    Peak Memory:             22.58 MB

  Parallelization Metrics:
    Number of Processes:     5
    Speedup:                 0.00x
    Parallel Efficiency:     0.00%
    Load Imbalance:          4.51%

  Accuracy:
    Correct Predictions:     9732 / 10000
    Accuracy:                97.32%
========================================================================

DETAILED ANALYSIS:
------------------
  Computation Time:        1.235 seconds (95.3%)
  Communication Overhead:  0.061 seconds (4.7%)
  Ideal Time (Perfect Scaling): inf seconds
  Scaling Loss:            -inf seconds

  Time Distribution:

Load Balancing Analysis:
  Max Process Time:        1.296 seconds
  Min Process Time:        1.238 seconds
  Time Variance:           0.058 seconds
  Load Imbalance Factor:   4.51%

  âœ“ Excellent load balance (< 5% imbalance)

[0;32mâœ“ Data parallel with 5 processes completed[0m
  â†’ Speedup: 4.07x, Efficiency: 80.0%


[1;33mTesting with 6 processes...[0m
i=0
i=1000


========================================================================
  DATA PARALLEL INFERENCE - PERFORMANCE SUMMARY
========================================================================
  Execution Metrics:
    Total Time:              1.177 seconds
    Inference Time:          1.164 seconds
    Model Load Time:         0.012 seconds
    Data Load Time:          0.002 seconds
    Communication Time:      0.042 seconds (3.5%)
      - MPI Send Time:       0.000 seconds
      - MPI Recv Time:       0.000 seconds
      - MPI Wait Time:       0.000 seconds

  Layer-wise Timing:

  Throughput & Latency:
    Throughput:              8590.98 images/second
    Avg Latency per Image:   0.116 ms
    Min Latency:             0.569 ms
    Max Latency:             6.324 ms

  Memory Usage:
    Peak Memory:             22.53 MB

  Parallelization Metrics:
    Number of Processes:     6
    Speedup:                 0.00x
    Parallel Efficiency:     0.00%
    Load Imbalance:          3.65%

  Accuracy:
    Correct Predictions:     9732 / 10000
    Accuracy:                97.32%
========================================================================

DETAILED ANALYSIS:
------------------
  Computation Time:        1.122 seconds (96.4%)
  Communication Overhead:  0.042 seconds (3.6%)
  Ideal Time (Perfect Scaling): inf seconds
  Scaling Loss:            -inf seconds

  Time Distribution:

Load Balancing Analysis:
  Max Process Time:        1.164 seconds
  Min Process Time:        1.121 seconds
  Time Variance:           0.043 seconds
  Load Imbalance Factor:   3.65%

  âœ“ Excellent load balance (< 5% imbalance)

[0;32mâœ“ Data parallel with 6 processes completed[0m
  â†’ Speedup: 4.53x, Efficiency: 70.0%


[1;33mTesting with 7 processes...[0m
i=0
i=1000


========================================================================
  DATA PARALLEL INFERENCE - PERFORMANCE SUMMARY
========================================================================
  Execution Metrics:
    Total Time:              1.078 seconds
    Inference Time:          1.064 seconds
    Model Load Time:         0.013 seconds
    Data Load Time:          0.001 seconds
    Communication Time:      0.020 seconds (1.9%)
      - MPI Send Time:       0.000 seconds
      - MPI Recv Time:       0.000 seconds
      - MPI Wait Time:       0.000 seconds

  Layer-wise Timing:

  Throughput & Latency:
    Throughput:              9394.28 images/second
    Avg Latency per Image:   0.106 ms
    Min Latency:             0.569 ms
    Max Latency:             5.330 ms

  Memory Usage:
    Peak Memory:             22.67 MB

  Parallelization Metrics:
    Number of Processes:     7
    Speedup:                 0.00x
    Parallel Efficiency:     0.00%
    Load Imbalance:          6.87%

  Accuracy:
    Correct Predictions:     9732 / 10000
    Accuracy:                97.32%
========================================================================

DETAILED ANALYSIS:
------------------
  Computation Time:        1.044 seconds (98.1%)
  Communication Overhead:  0.020 seconds (1.9%)
  Ideal Time (Perfect Scaling): inf seconds
  Scaling Loss:            -inf seconds

  Time Distribution:

Load Balancing Analysis:
  Max Process Time:        1.064 seconds
  Min Process Time:        0.991 seconds
  Time Variance:           0.073 seconds
  Load Imbalance Factor:   6.87%

  âš  Good load balance (< 15% imbalance)

[0;32mâœ“ Data parallel with 7 processes completed[0m
  â†’ Speedup: 4.95x, Efficiency: 70.0%


[1;33mTesting with 8 processes...[0m
i=0
i=1000


========================================================================
  DATA PARALLEL INFERENCE - PERFORMANCE SUMMARY
========================================================================
  Execution Metrics:
    Total Time:              0.991 seconds
    Inference Time:          0.975 seconds
    Model Load Time:         0.012 seconds
    Data Load Time:          0.002 seconds
    Communication Time:      0.022 seconds (2.2%)
      - MPI Send Time:       0.000 seconds
      - MPI Recv Time:       0.000 seconds
      - MPI Wait Time:       0.000 seconds

  Layer-wise Timing:

  Throughput & Latency:
    Throughput:              10252.38 images/second
    Avg Latency per Image:   0.098 ms
    Min Latency:             0.569 ms
    Max Latency:             16.672 ms

  Memory Usage:
    Peak Memory:             22.69 MB

  Parallelization Metrics:
    Number of Processes:     8
    Speedup:                 0.00x
    Parallel Efficiency:     0.00%
    Load Imbalance:          4.93%

  Accuracy:
    Correct Predictions:     9732 / 10000
    Accuracy:                97.32%
========================================================================

DETAILED ANALYSIS:
------------------
  Computation Time:        0.953 seconds (97.7%)
  Communication Overhead:  0.022 seconds (2.3%)
  Ideal Time (Perfect Scaling): inf seconds
  Scaling Loss:            -inf seconds

  Time Distribution:

Load Balancing Analysis:
  Max Process Time:        0.975 seconds
  Min Process Time:        0.927 seconds
  Time Variance:           0.048 seconds
  Load Imbalance Factor:   4.93%

  âœ“ Excellent load balance (< 5% imbalance)

[0;32mâœ“ Data parallel with 8 processes completed[0m
  â†’ Speedup: 5.41x, Efficiency: 60.0%

[0;34m[Step 4/5] Running Pipeline Parallel Inference (MPI)...[0m
==================================================================================

[1;33mTesting with 5 processes (5-stage pipeline)...[0m
in cpu 3
in cpu 4
in cpu 2
in cpu 1
in cpu 0
in cpu 0 done
in cpu 1 done
in cpu 2 done
in cpu 3 done
in cpu 4  done
ntests=10000, ncorrect=9732
Total correct predictions: 9732
Total execution time: 3.367824 seconds
[0;32mâœ“ Pipeline parallel with 5 processes completed[0m
  â†’ Speedup: 1.56x, Efficiency: 30.0%

[0;34m[Step 5/5] Generating Comparative Analysis...[0m
==================================================================================
[0;32mâœ“ Comparative analysis generated[0m

================================================================================
                    ENHANCED BENCHMARK COMPLETED                               
================================================================================

Results saved to: results/benchmark_results_detailed.txt

Performance dimensions tracked:
  âœ“ Execution time (total, inference, communication)
  âœ“ Throughput (images/second)
  âœ“ Latency (min, max, average)
  âœ“ Memory usage (peak consumption)
  âœ“ Parallelization metrics (speedup, efficiency)
  âœ“ Load balancing (data parallel)
  âœ“ Communication overhead (MPI)

Summary:
  - Serial baseline: 5.276 seconds
  - Data parallel tested: 1P, 2P, 3P, 4P, 5P, 6P, 7P, 8P (all 8 cores)
  - Pipeline parallel: 3.367824 seconds (5P)

Next steps:
  1. Review results/benchmark_results_detailed.txt for detailed multi-dimensional analysis
  2. Compare metrics across implementations
  3. Identify bottlenecks (communication, load imbalance, memory)
  4. Optimize based on insights

================================================================================
[1m[7m%[27m[1m[0m                                                                                                                                            [0m[27m[24m[Jsaurabh@sORo cnn-parallelism % [K[?2004hmmam  [7mmake analyze [27m[13D[27mm[27ma[27mk[27me[27m [27ma[27mn[27ma[27ml[27my[27mz[27me[27m [?2004l

====================================================================================================
                              PERFORMANCE METRICS SUMMARY
====================================================================================================
Implementation            Processes  Time(s)    Throughput      Speedup    Efficiency   Memory(MB)  
----------------------------------------------------------------------------------------------------
Serial                    1          5.276      1895.4 img/s    1.00x      100.0%       11.8        
Data Parallel (2P)        2          2.767      3614.0 img/s    1.91x      95.3%        22.4        
Data Parallel (3P)        3          1.912      5230.2 img/s    2.76x      92.0%        22.6        
Data Parallel (4P)        4          1.440      6942.9 img/s    3.66x      91.6%        22.6        
Data Parallel (5P)        5          1.296      7716.5 img/s    4.07x      81.4%        22.6        
Data Parallel (6P)        6          1.164      8591.0 img/s    4.53x      75.5%        22.5        
Data Parallel (7P)        7          1.064      9394.3 img/s    4.96x      70.8%        22.7        
Data Parallel (8P)        8          0.975      10252.4 img/s   5.41x      67.6%        22.7        
Pipeline (5P)             5          3.368      N/A             1.57x      31.3%        0.0         
====================================================================================================

====================================================================================================
                                   DETAILED ANALYSIS
====================================================================================================

1. SCALING EFFICIENCY ANALYSIS
--------------------------------------------------
  2 Processes: Speedup=1.91x, Efficiency=95.3%
    Status: âœ“ Excellent

  3 Processes: Speedup=2.76x, Efficiency=92.0%
    Status: âœ“ Excellent
    Scaling from 2P to 3P: 1.45x

  4 Processes: Speedup=3.66x, Efficiency=91.6%
    Status: âœ“ Excellent
    Scaling from 3P to 4P: 1.33x

  5 Processes: Speedup=4.07x, Efficiency=81.4%
    Status: âœ“ Good
    Scaling from 4P to 5P: 1.11x

  6 Processes: Speedup=4.53x, Efficiency=75.5%
    Status: âœ“ Good
    Scaling from 5P to 6P: 1.11x

  7 Processes: Speedup=4.96x, Efficiency=70.8%
    Status: âš  Fair
    Scaling from 6P to 7P: 1.09x

  8 Processes: Speedup=5.41x, Efficiency=67.6%
    Status: âš  Fair
    Scaling from 7P to 8P: 1.09x


2. LATENCY ANALYSIS
--------------------------------------------------
  Serial:
    Min: 0.520 ms
    Avg: 0.528 ms
    Max: 1.109 ms
    Variance: 0.589 ms

  Data Parallel (2P):
    Min: 0.548 ms
    Avg: 0.277 ms
    Max: 0.977 ms
    Variance: 0.429 ms

  Data Parallel (3P):
    Min: 0.569 ms
    Avg: 0.191 ms
    Max: 1.121 ms
    Variance: 0.552 ms

  Data Parallel (4P):
    Min: 0.569 ms
    Avg: 0.144 ms
    Max: 1.678 ms
    Variance: 1.109 ms

  Data Parallel (5P):
    Min: 0.569 ms
    Avg: 0.130 ms
    Max: 4.138 ms
    Variance: 3.569 ms

  Data Parallel (6P):
    Min: 0.569 ms
    Avg: 0.116 ms
    Max: 6.324 ms
    Variance: 5.755 ms

  Data Parallel (7P):
    Min: 0.569 ms
    Avg: 0.106 ms
    Max: 5.330 ms
    Variance: 4.761 ms

  Data Parallel (8P):
    Min: 0.569 ms
    Avg: 0.098 ms
    Max: 16.672 ms
    Variance: 16.103 ms


3. COMMUNICATION OVERHEAD ANALYSIS
--------------------------------------------------
  Data Parallel (2P):
    Computation: 100.0%
    Communication: 0.0%
    â†’ Excellent (<5% overhead)

  Data Parallel (3P):
    Computation: 100.0%
    Communication: 0.0%
    â†’ Excellent (<5% overhead)

  Data Parallel (4P):
    Computation: 99.7%
    Communication: 0.3%
    â†’ Excellent (<5% overhead)

  Data Parallel (5P):
    Computation: 95.3%
    Communication: 4.7%
    â†’ Excellent (<5% overhead)

  Data Parallel (6P):
    Computation: 96.4%
    Communication: 3.6%
    â†’ Excellent (<5% overhead)

  Data Parallel (7P):
    Computation: 98.1%
    Communication: 1.9%
    â†’ Excellent (<5% overhead)

  Data Parallel (8P):
    Computation: 97.7%
    Communication: 2.3%
    â†’ Excellent (<5% overhead)


4. LOAD BALANCING ANALYSIS
--------------------------------------------------
  Data Parallel (2P): 0.1% imbalance
    âœ“ Excellent load distribution

  Data Parallel (3P): 0.1% imbalance
    âœ“ Excellent load distribution

  Data Parallel (4P): 0.3% imbalance
    âœ“ Excellent load distribution

  Data Parallel (5P): 4.5% imbalance
    âœ“ Excellent load distribution

  Data Parallel (6P): 3.6% imbalance
    âœ“ Excellent load distribution

  Data Parallel (7P): 6.9% imbalance
    âœ“ Good load distribution

  Data Parallel (8P): 4.9% imbalance
    âœ“ Excellent load distribution


5. MEMORY EFFICIENCY ANALYSIS
--------------------------------------------------
  Serial baseline: 11.8 MB
  Data Parallel (2P): 22.4 MB total, 11.2 MB/process
    Memory overhead vs serial: 90.2%
  Data Parallel (3P): 22.6 MB total, 7.5 MB/process
    Memory overhead vs serial: 91.7%
  Data Parallel (4P): 22.6 MB total, 5.6 MB/process
    Memory overhead vs serial: 91.9%
  Data Parallel (5P): 22.6 MB total, 4.5 MB/process
    Memory overhead vs serial: 91.8%
  Data Parallel (6P): 22.5 MB total, 3.8 MB/process
    Memory overhead vs serial: 91.4%
  Data Parallel (7P): 22.7 MB total, 3.2 MB/process
    Memory overhead vs serial: 92.6%
  Data Parallel (8P): 22.7 MB total, 2.8 MB/process
    Memory overhead vs serial: 92.8%

6. LAYER-WISE COMPUTATION BREAKDOWN
--------------------------------------------------

====================================================================================================
                                   OPTIMIZATION RECOMMENDATIONS
====================================================================================================

âœ“ Best configuration: Data Parallel with 8 processes
  Speedup: 5.41x
  Efficiency: 67.6%

âš  Efficiency drops significantly at higher process counts
  Recommendation: Use fewer processes for better efficiency
  Optimal: 8 processes

âš  Pipeline parallel shows low efficiency
  Reason: High communication overhead from layer-to-layer transfers
  Recommendation: Use data parallel for inference workloads

====================================================================================================

Analysis complete!
Full results available in: results/benchmark_results_detailed.txt
[1m[7m%[27m[1m[0m                                                                                                                                            [0m[27m[24m[Jsaurabh@sORo cnn-parallelism % [K[?2004h